{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome!\n",
        "\n",
        "## Plan for this workshop:\n",
        "\n",
        "- Import dataset\n",
        "- Text Preprocessing\n",
        "- Working with Datasets\n",
        "- Tokenization\n",
        "- Data Preparation\n",
        "- Classification"
      ],
      "metadata": {
        "id": "E_lVz1O97EOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is NLP?\n",
        "NLP, or Natural Language Processing, is a subfield of artificial\n",
        "intelligence (AI) that deals with the interaction between computers and\n",
        "humans in natural language.\n",
        "\n",
        "## Usages of NLP\n",
        "NLP has numerous applications across various industries:\n",
        "\n",
        "- Sentiment Analysis: Determining the emotional tone of text, such as identifying positive, negative, or neutral sentiments in customer reviews.\n",
        "- Chatbots: Creating conversational agents that can interact with users in a human-like manner.\n",
        "- Machine Translation: Automatically translating text from one language to another.\n",
        "- Text Summarization: Condensing large amounts of text into shorter, concise summaries.\n",
        "- Question Answering: Developing systems that can answer questions posed in natural language.\n",
        "\n"
      ],
      "metadata": {
        "id": "WpibTmPRyLIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine translation influenced the development of NLP\n",
        "\n",
        "- Early Days: In the early days of computing, machine translation was a primary focus. Researchers sought to develop algorithms and techniques to automatically translate between languages.\n",
        "- Rule-Based Approaches: Initial efforts relied on rule-based approaches, where systems followed predefined grammatical rules to convert text. However, these methods proved limited in handling the nuances and complexities of human language.\n",
        "- Statistical Methods: The field shifted towards statistical methods, leveraging large amounts of parallel text data to learn patterns and probabilities for translation.\n",
        "- Foundation for NLP: These statistical techniques, along with linguistic concepts developed for machine translation, formed the foundation for many core NLP tasks like part-of-speech tagging, parsing, and text analysis.\n",
        "- Modern NLP: While machine translation remains a key area within NLP, the field has expanded to encompass a wider range of applications, including sentiment analysis, text summarization, question answering, and more."
      ],
      "metadata": {
        "id": "-nViwVVxyY2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example using Transformers\n",
        "\n",
        "### Sentiment classification üíó\n",
        "\n",
        "Here's a simple example of **sentiment analysis** using the Hugging Face Transformers library:\n",
        "\n",
        "First, install the transformers library:"
      ],
      "metadata": {
        "id": "E9ZOQ7Zuyla7"
      }
    },
    {
      "source": [
        "!pip install transformers -q"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "x6a8pljAypv6"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets pyarrow==14.0.2"
      ],
      "metadata": {
        "id": "CJCS_bHRJJkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, run the following code:\n",
        "\n"
      ],
      "metadata": {
        "id": "ESH1bu82yr5N"
      }
    },
    {
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"This is a positive sentence.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "XZezGWewytaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "BTzOBNGWtBhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet uses a pre-trained sentiment analysis model to classify the sentiment of the input sentence."
      ],
      "metadata": {
        "id": "8i4Ap5QQywGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text generation üìö"
      ],
      "metadata": {
        "id": "pDZ8ukZYkAX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It involves using models to create new text, such as completing sentences, writing stories, or generating different creative content formats.\n",
        "\n",
        "Here's an example of how to perform text generation using the Hugging Face Transformers library:"
      ],
      "metadata": {
        "id": "stjbEe7TzIrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generator takes user input, processes it with the model, and returns a generated response. Feel free to modify the user_input and experiment with different interactions."
      ],
      "metadata": {
        "id": "jr3cJirjzACw"
      }
    },
    {
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "result = generator(\"We are at a hackers conference  \", max_length=50)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zQIYMYwVzKEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "4Wmw2l6mtYCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Chat üêà"
      ],
      "metadata": {
        "id": "VK1j-ASFkRZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the text generation pipeline with a dialog model\n",
        "generator = pipeline(\"text-generation\", model=\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "# Function to generate chatbot responses\n",
        "def generate_response(user_input):\n",
        "  response = generator(user_input, max_length=1000)[0][\"generated_text\"]\n",
        "  return response\n",
        "\n",
        "user_input = \"Hi\"\n",
        "\n",
        "response = generate_response(user_input)"
      ],
      "metadata": {
        "id": "PiLZs1y_klpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)\n"
      ],
      "metadata": {
        "id": "spigSd-gtnVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP pipeline\n"
      ],
      "metadata": {
        "id": "ggChDLxhvWhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "- Load Data: Import dataset from Hugging Face Datasets.\n",
        "- Text Preprocessing:\n",
        "  - Lowercasing\n",
        "  - Punctuation removal\n",
        "- Tokenization: Utilize a pre-trained tokenizer from Transformers\n",
        "- Data Preparation:\n",
        "  - Create vocabulary\n",
        "  - Convert text to numerical vectors\n",
        "  - Classification\n",
        "\n",
        "\n",
        "  ![Pipeline](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)"
      ],
      "metadata": {
        "id": "O7ax-eSo6rbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data import and cleaning\n",
        "\n"
      ],
      "metadata": {
        "id": "21Qecb7t2CN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing a dataset üì´:"
      ],
      "metadata": {
        "id": "EI2tOrjtnA-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "YE4qeRT26ebD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the IMDB dataset: [model card](https://huggingface.co/datasets/stanfordnlp/imdb)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IBaQHjcV2Cq7"
      }
    },
    {
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"imdb\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2i5k-iAD2Qx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "dMJFDLAKTnfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "I6yhLopbT4sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = dataset[\"train\"].to_pandas()\n",
        "df"
      ],
      "metadata": {
        "id": "VB4uIzKeTmuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning üßπ\n",
        "\n",
        "Now that we have the movie review dataset, let's perform some cleaning on the text data before tokenization. We'll focus on **lowercasing** and **removing punctuation** for this example."
      ],
      "metadata": {
        "id": "WEYHWueI2iKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'][0]"
      ],
      "metadata": {
        "id": "SQb4qDHP7GfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  text = text.lower()  # Lowercase the text\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "  return text\n",
        "\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "print(df.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RjV7Oxcm2jc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'][0]"
      ],
      "metadata": {
        "id": "8v0USFrNTFK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a clean_text function that takes a text string as input, converts it to lowercase using .lower(), and removes punctuation using a regular expression (re.sub()). We then apply this function to the 'text' column of the DataFrame using .apply()."
      ],
      "metadata": {
        "id": "tuQhtVc62nWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's also remove stop words: ‚èπ"
      ],
      "metadata": {
        "id": "hBlpexzW-9bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "text = df['text'][0]\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokens = text.split()\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "vQoCvvs0_AEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_text = ' '.join(filtered_tokens)\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "id": "YbRJSLfNzfIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "\n",
        "Lemmatization and stemming are both text normalization techniques used in natural language processing (NLP) to prepare text for analysis. They aim to reduce words to their base or root form, but they differ in their approach and results.\n",
        "\n",
        "**Stemming**\n",
        "\n",
        "Chops off the ends of words to remove suffixes, prefixes, etc.\n",
        "It reduces words to their stem (root form).\n",
        "Example: \"running\" becomes \"run,\" \"studies\" becomes \"studi\".\n",
        "\n",
        "**Lemmatization**\n",
        "\n",
        "Reduces words to their dictionary form (lemma).\n",
        "Example: \"better\" becomes \"good,\" \"running\" becomes \"run.\"\n"
      ],
      "metadata": {
        "id": "yOlucDrH0MMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "HyIs_wT-0PzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "for token in stemmed_tokens:\n",
        "  print(token)\n"
      ],
      "metadata": {
        "id": "3kxgovmmzmyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "for token in lemmatized_tokens:\n",
        "  print(token)\n"
      ],
      "metadata": {
        "id": "uDaqyc_m1ZPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization: Turning Words into Numbers\n",
        "While humans understand words and sentences, computers primarily work with numbers. To enable computers to process and understand human language, we need a way to convert text into a numerical representation. This is where tokenization comes in.\n",
        "\n",
        "Tokenization is the process of breaking down text into individual units called tokens. These tokens can be words, subwords, or even characters, depending on the chosen approach.\n",
        "\n",
        "For instance, the sentence \"This is an example.\" can be tokenized into the following words:\n",
        "\n",
        "[\"This\", \"is\", \"an\", \"example\", \".\"]\n",
        "\n",
        "Each token is then assigned a unique numerical identifier, allowing computers to represent and manipulate text using numbers.\n",
        "\n",
        "Here's a simple example of tokenization using the Hugging Face Transformers library:"
      ],
      "metadata": {
        "id": "8oykjYhhzmsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#just taking our previous sentence for later:\n",
        "example1 = df['text'][0]"
      ],
      "metadata": {
        "id": "cfP2LN4S8OZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokens = tokenizer.tokenize(\"We are at a hacking conference.\")\n",
        "print(tokens)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "K0tQiAhXzn6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "lem_tokens = [tokenizer.tokenize(token) for token in lemmatized_tokens]\n",
        "\n",
        "print(lem_tokens)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "n9wMAsLZ2kxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization Examples\n",
        "Let's explore different tokenization methods and their importance:\n",
        "\n",
        "1. Basic Splitting\n",
        "\n",
        "A simple way to tokenize text is by splitting it based on spaces:"
      ],
      "metadata": {
        "id": "8vncccfwz1Q2"
      }
    },
    {
      "source": [
        "\n",
        "hacking_sent = \"We are at a hacking conference.\"\n",
        "#text = df['text'][0]\n",
        "#tokens = text.split()\n",
        "tokens = hacking_sent.split()\n",
        "print(tokens)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wM3BF2euz2Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example uses a regular expression to extract words while ignoring punctuation.\n",
        "\n",
        "2. Transformers Tokenizers\n",
        "\n",
        "For advanced NLP tasks, using tokenizers from pre-trained models is crucial:"
      ],
      "metadata": {
        "id": "L2P2pn0mz_W6"
      }
    },
    {
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokens = tokenizer.tokenize(hacking_sent)\n",
        "print(tokens)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CgJU6ZHk0F3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokens to ids üî¢"
      ],
      "metadata": {
        "id": "U4nzA9n09KdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The token ids are the unique value of a token in a vocabulary list."
      ],
      "metadata": {
        "id": "Ql5ckqJ625BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens)\n",
        "print(token_ids)\n"
      ],
      "metadata": {
        "id": "QK47JJ_d9I7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokens = tokenizer.tokenize(\"We are at a hacking party.\")\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens)\n",
        "print(token_ids)\n"
      ],
      "metadata": {
        "id": "fLugBLp59N1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(\"You are at a hacking party.\")\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "id": "7ZPRKycG9jpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(\"You are at a hacking party!!\")\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "id": "MIjIjTj29sg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These tokenizers are trained on large datasets and can handle various linguistic nuances, including subword tokenization and special characters.\n",
        "\n"
      ],
      "metadata": {
        "id": "XbutM9on0Iz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding and Truncation üî™\n",
        "\n",
        "When working with Transformers for NLP tasks, we often deal with multiple sentences or text sequences of varying lengths. However, these models typically require input sequences to have the same length. This is where padding and truncation come into play.\n",
        "\n",
        "### **Padding**\n",
        "\n",
        "Padding involves adding special tokens (e.g., [PAD]) to shorter sequences to make them match the length of the longest sequence in a batch. This ensures consistent input dimensions for the model.\n",
        "\n",
        "### **Truncation**\n",
        "\n",
        "Truncation involves shortening longer sequences to a maximum length. This can be done by removing tokens from the beginning or end of the sequence.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose we have two sentences:\n",
        "\n",
        "- Sentence 1: \"This is a short sentence.\"\n",
        "- Sentence 2: \"This is a much longer sentence with more words.\"\n",
        "\n",
        "After tokenization, we might have:\n",
        "\n",
        "- Sentence 1: [\"This\", \"is\", \"a\", \"short\", \"sentence\", \".\"]\n",
        "- Sentence 2: [\"This\", \"is\", \"a\", \"much\", \"longer\", \"sentence\", \"with\", \"more\", \"words\", \".\"]\n",
        "\n",
        "To process these sentences with a Transformer, we might set a maximum length of 8 tokens.\n",
        "\n",
        "- Padding: Sentence 1 would be padded with two [PAD] tokens: [\"This\", \"is\", \"a\", \"short\", \"sentence\", \".\", [PAD], [PAD]]\n",
        "- Truncation: Sentence 2 would be truncated to 8 tokens: [\"This\", \"is\", \"a\", \"much\", \"longer\", \"sentence\", \"with\"]\n",
        "\n",
        "Proper padding and truncation can prevent errors and improve the performance of Transformer models."
      ],
      "metadata": {
        "id": "cWURnEgD0ghW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"A white rabbit.\", \"A lot of black cats in the garden.\"]\n",
        "\n",
        "padded_sequences = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors=\"pt\") #pt for pyTorch"
      ],
      "metadata": {
        "id": "IWgDD5wrRg0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences"
      ],
      "metadata": {
        "id": "lZqNUdHASGGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input_ids_tensor1 = padded_sequences['input_ids'][0]\n",
        "input_ids_tensor2 = padded_sequences['input_ids'][1]\n",
        "\n",
        "print(\"Input IDs Tensor 1:\\n\", input_ids_tensor1)\n",
        "print(\"\\nInput IDs Tensor 2:\\n\", input_ids_tensor2)\n",
        "\n",
        "combined_tensor = torch.cat((input_ids_tensor1.unsqueeze(0), input_ids_tensor2.unsqueeze(0)), dim=0)\n",
        "print(\"\\nCombined Tensor:\\n\", combined_tensor)\n"
      ],
      "metadata": {
        "id": "AyjlD99rSNb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a tokenization function and add a new column to our DataFrame with the tokens for each review. We'll use a basic tokenizer from the Hugging Face Transformers library for this example."
      ],
      "metadata": {
        "id": "FGGSnKpC23Hu"
      }
    },
    {
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_text(text):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  return tokens\n",
        "\n",
        "df['tokens'] = df['text'].apply(tokenize_text)\n",
        "print(df.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "f-52sOL8249-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokens']"
      ],
      "metadata": {
        "id": "uHlIi6GY0ZLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "cNOvXEFTTRL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis\n",
        "\n",
        "Let's focus on a classification task using the first 5 lines of our processed and tokenized movie review data. For simplicity, we'll perform sentiment analysis and try to predict whether a review is positive or negative.\n"
      ],
      "metadata": {
        "id": "PMsHqXXv3U-9"
      }
    },
    {
      "source": [
        "data = df[['tokens', 'label']].head(5)\n",
        "data"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JcUzccFX3aWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can perform sentiment classification on each of the first 5 movie reviews using a pre-trained model. Since we have limited data, using a pre-trained model is a good approach as it leverages knowledge learned from a massive dataset."
      ],
      "metadata": {
        "id": "zL-NAEee3t7X"
      }
    },
    {
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "data = df['text'].head(5)\n",
        "sentiments = data.apply(lambda text: classifier(text)[0]['label'])\n",
        "print(sentiments)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ZSa_NxyGAsn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty bad results üò∂\n",
        "\n",
        "Here might be why (01:19 am reflexion):\n",
        "\n",
        "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model **will result in indexing errors**.\n"
      ],
      "metadata": {
        "id": "Rw0LCjVbr5pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_tweet_tokens = df['tokens'][2]\n",
        "num_tokens = len(first_tweet_tokens)\n",
        "print(f\"Number of tokens in the first tweet: {num_tokens}\")\n"
      ],
      "metadata": {
        "id": "QeaKiKnG47jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ü§î 01:22 am..."
      ],
      "metadata": {
        "id": "aNfRjXVD5OfC"
      }
    },
    {
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"bert-base-uncased\"  # Example model, choose one suitable for your task\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "def predict_sentiment(tokens):\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  input_ids = torch.tensor([input_ids])  # Convert to PyTorch tensor\n",
        "  outputs = model(input_ids)\n",
        "  predicted_class = outputs.logits.argmax(-1)\n",
        "  return predicted_class\n",
        "\n",
        "data = df['tokens'].head(5)\n",
        "sentiments = data.apply(predict_sentiment)\n",
        "print(sentiments)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "B7mKqnUfAXH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üòÄ 01:24 am\n",
        "\n",
        "It looks better!!"
      ],
      "metadata": {
        "id": "DEWbB1Jj5kjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your turn"
      ],
      "metadata": {
        "id": "dNwyPaJe4CBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing a Sentence\n",
        "\n",
        "Exercise:\n",
        "\n",
        "- Use the BERT tokenizer to tokenize the third sentence of the dataset.\n",
        "- Store the tokens in a variable called \"tokens\" and print them."
      ],
      "metadata": {
        "id": "A61bcNYP4OhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Solution):\n",
        "\n"
      ],
      "metadata": {
        "id": "HQfklYo04bIT"
      }
    },
    {
      "source": [
        "sentence = df['text'][2]\n",
        "print(sentence)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MlkZF0CJ4dI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "RP3PCVSWDa6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Stop Words\n",
        "\n",
        "- Remove stop words from the following sentence: \"This is a sentence with some stop words.\"\n",
        "- Use the NLTK library and its stopwords corpus.\n",
        "- Store the filtered tokens in a variable called \"filtered_tokens\" and print them."
      ],
      "metadata": {
        "id": "nmgfJ97m4gFg"
      }
    },
    {
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xKR15auf401q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sentence.split()\n",
        "tokens"
      ],
      "metadata": {
        "id": "iQwJRg7qDi0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "Gno9plvuDlOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Calculating Sentiment Score\n",
        "\n",
        "- Use a pre-trained sentiment analysis model from the transformers library to calculate the sentiment of the sentence for your sentence.\n",
        "- Print the result, which should include the predicted label and score."
      ],
      "metadata": {
        "id": "Ut0SNnbv43Au"
      }
    },
    {
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(sentence)\n",
        "print(result)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WbFJxAdq47HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What we covered\n",
        "\n",
        "- What is NLP?\n",
        "- Applications of NLP\n",
        "- Transformers Library\n",
        "- Tokenization\n",
        "- Text Preprocessing\n",
        "- Padding and Truncation\n",
        "- Run a sentiment classifer\n",
        "\n",
        "## Now your data is ready for training!\n",
        "## Let's go deeper.."
      ],
      "metadata": {
        "id": "WAQricK75JVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions on a real dataset"
      ],
      "metadata": {
        "id": "cRnzM24bQ4dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "MdH4XV3iROFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_sample = df.head(10)"
      ],
      "metadata": {
        "id": "E0oVYlTCRQu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "TFHTeLjI7fqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh, only '0' labels! üòØ"
      ],
      "metadata": {
        "id": "do6D_qrr6595"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I want to create a subset of df which has both  text labeled as 0 and 1. Create a subset of 10\n",
        "\n",
        "df_subset = df[df.label.isin([0, 1])].groupby('label').head(5)\n",
        "\n",
        "df_subset"
      ],
      "metadata": {
        "id": "8VdA0v9dVcuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the DataFrame rows\n",
        "shuffled_df = df_subset.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(shuffled_df)\n"
      ],
      "metadata": {
        "id": "C1nAAkoiV4UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show the first 4 tweets of this dataframe"
      ],
      "metadata": {
        "id": "Zau3lD9oRdRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_df[:4]"
      ],
      "metadata": {
        "id": "90tuXXJeRd69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show only the labels for the first 4 tweets"
      ],
      "metadata": {
        "id": "2Lie_nHKRgos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_df[:4]['label']"
      ],
      "metadata": {
        "id": "EC7dbzJURjwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict sentiment"
      ],
      "metadata": {
        "id": "8K7CMLxbRpS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenized_tweets = shuffled_df[\"text\"].apply(lambda text: tokenizer(text, padding=\"max_length\", truncation=True)).tolist()\n",
        "\n",
        "# Convert tokenized data into PyTorch tensors\n",
        "input_ids = [tweet['input_ids'] for tweet in tokenized_tweets]\n",
        "attention_mask = [tweet['attention_mask'] for tweet in tokenized_tweets]\n",
        "\n",
        "input_ids = torch.tensor(input_ids)\n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "\n",
        "# Run the model for prediction\n",
        "with torch.no_grad():\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Get the predicted labels\n",
        "predicted_labels = torch.argmax(outputs.logits, dim=1).numpy()\n",
        "\n",
        "# Add the predicted sentiment to the sample DataFrame\n",
        "shuffled_df[\"predicted_sentiment\"] = predicted_labels\n",
        "\n",
        "shuffled_df"
      ],
      "metadata": {
        "id": "kID7BW2ERxGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(df):\n",
        "  correct_predictions = (df['label'] == df['predicted_sentiment']).sum()\n",
        "  total_predictions = len(df)\n",
        "  accuracy = correct_predictions / total_predictions\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "6DbnQ_ETXkAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = calculate_accuracy(shuffled_df)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "04uMHTzBedWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to improve accuracy?"
      ],
      "metadata": {
        "id": "fVKgrAFoYZtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Name entity recognition"
      ],
      "metadata": {
        "id": "DzZfaeP_Qe-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are entities? üëΩ\n",
        "\n",
        "BERT-base-NER, fine-tuned for Named Entity Recognition (NER), typically uses the following entities:\n",
        "\n",
        "**Common Entities:**\n",
        "\n",
        "- PER: Person\n",
        "- ORG: Organization\n",
        "- LOC: Location\n",
        "- MISC: Miscellaneous\n",
        "- Less Common, but Sometimes Included:\n",
        "\n",
        "- DATE: Date\n",
        "- TIME: Time\n",
        "- MONEY: Monetary values\n",
        "- PERCENT: Percentage\n",
        "- Important Notes:\n",
        "\n",
        "\n",
        "**B-, I- Prefixes:** You'll often see these prefixes before entity labels. They indicate the beginning (B-) and inside (I-) of a multi-word entity. For example:\n",
        "B-PER (Beginning of a person's name)\n",
        "I-PER (Inside a person's name)\n"
      ],
      "metadata": {
        "id": "d5phift0f2yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']['text'][0:2]"
      ],
      "metadata": {
        "id": "dFUyWCNY-mVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Choose a pre-trained model for NER\n",
        "model_name = \"dslim/bert-base-NER\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "NER_model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create a NER pipeline\n",
        "nlp = pipeline(\"ner\", model=NER_model, tokenizer=tokenizer)\n",
        "\n",
        "# Take the first five tweets from the dataset for NER\n",
        "subset_datasets = dataset[\"train\"].select(range(5))\n",
        "\n",
        "# Process and print NER results for each example\n",
        "for example in subset_datasets:\n",
        "  text = example[\"text\"]\n",
        "  print(f\"Text: {text}\")\n",
        "  ner_results = nlp(text)\n",
        "  print(f\"NER Results: {ner_results}\")\n",
        "  print(\"---\")\n"
      ],
      "metadata": {
        "id": "JNtCqJM4NHTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example in subset_datasets:\n",
        "  text = example[\"text\"]\n",
        "  short_text = ' '.join(text.split()[:30])\n",
        "  print(f\"Text: {short_text}\")\n",
        "  ner_results = nlp(text)\n",
        "  print(\"Token\\tNER\")\n",
        "  for result in ner_results:\n",
        "    print(f\"{result['word']}\\t{result['entity']}\")\n",
        "  print(\"---\")"
      ],
      "metadata": {
        "id": "UJ8q6dkpAGjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrap it all up üëâ an example of dialog generation"
      ],
      "metadata": {
        "id": "Yd5D4JH-lDPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate chatbot responses\n",
        "def generate_response(user_input):\n",
        "  input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n",
        "  chat_history_ids = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "  response = tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "  return response\n",
        "\n",
        "\n",
        "user_input = \"Thanks guys, this is the end of this workshop. Hope you enjoyed it!\"\n",
        "response = generate_response(user_input)\n"
      ],
      "metadata": {
        "id": "g3ah7eqweaVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "qRpWvR54AkIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Questions ?\n",
        "\n",
        "\n",
        "# Thank you!\n",
        "\n",
        "üßë\n",
        "Twitter: @hello_locked | C00kie_two@infosec.exchange\n"
      ],
      "metadata": {
        "id": "_7qhV4EPlURd"
      }
    }
  ]
}
